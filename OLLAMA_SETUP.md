# راهنمای استفاده از Ollama در Locale Bulk Translator

## پیش‌نیازها

1. **نصب Ollama**: ابتدا Ollama را از [ollama.ai](https://ollama.ai) نصب کنید
2. **دانلود مدل**: مدل مورد نظر خود را دانلود کنید (مثل gemma3:4b)

## مراحل راه‌اندازی

### 1. نصب و راه‌اندازی Ollama

```bash
# نصب Ollama (macOS)
curl -fsSL https://ollama.ai/install.sh | sh

# راه‌اندازی سرویس Ollama
ollama serve

# دانلود مدل gemma3:4b
ollama pull gemma3:4b
```

### 2. تست اتصال به Ollama

```bash
# تست API
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3:4b",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### 3. تنظیمات در VS Code Extension

1. **باز کردن Command Palette**: `Cmd+Shift+P` (macOS) یا `Ctrl+Shift+P` (Windows/Linux)
2. **انتخاب دستور**: `I18n Nexus: Configure AI Model`
3. **انتخاب Provider**: `ollama`
4. **تنظیم Model**: `gemma3:4b` (یا هر مدل دیگری که نصب کرده‌اید)
5. **تنظیم API URL**: `http://localhost:11434/v1/chat/completions`
6. **API Key**: برای Ollama خالی بگذارید (نیاز نیست)

### 4. استفاده از Extension

پس از تنظیمات، می‌توانید از تمام قابلیت‌های ترجمه استفاده کنید:

- **ترجمه فایل‌های JSON**: انتخاب فایل و ترجمه به زبان هدف
- **ترجمه تدریجی**: ترجمه بخش‌بخش فایل‌های بزرگ
- **اعتبارسنجی ترجمه**: بررسی کیفیت ترجمه‌های انجام شده

## مدل‌های پیشنهادی برای ترجمه

### مدل‌های کوچک (سریع‌تر)
- `gemma2:2b` - سریع و کارآمد
- `llama2:7b` - تعادل خوب بین سرعت و کیفیت
- `mistral:7b` - کیفیت بالا برای اندازه کوچک

### مدل‌های متوسط
- `gemma3:4b` - کیفیت خوب و سرعت مناسب
- `llama2:13b` - کیفیت بالاتر
- `mistral:7b-instruct` - بهینه‌سازی شده برای دستورات

### مدل‌های بزرگ (کیفیت بالاتر)
- `llama2:70b` - بهترین کیفیت (نیاز به RAM بالا)
- `codellama:34b` - مناسب برای کد و متن

## عیب‌یابی

### مشکل: اتصال به Ollama برقرار نمی‌شود
```bash
# بررسی وضعیت سرویس
ollama list

# راه‌اندازی مجدد
ollama serve
```

### مشکل: مدل پیدا نمی‌شود
```bash
# لیست مدل‌های نصب شده
ollama list

# دانلود مجدد مدل
ollama pull gemma3:4b
```

### مشکل: خطای حافظه
- مدل کوچک‌تری انتخاب کنید
- RAM سیستم را بررسی کنید
- از مدل‌های quantized استفاده کنید (مثل `gemma3:4b-q4_K_M`)

## نکات مهم

1. **اولین اجرا**: اولین بار که مدل را استفاده می‌کنید، ممکن است کمی طول بکشد تا مدل بارگذاری شود
2. **حافظه**: مدل‌های بزرگتر نیاز به RAM بیشتری دارند
3. **سرعت**: مدل‌های محلی معمولاً کندتر از API های ابری هستند اما حریم خصوصی بیشتری دارند
4. **کیفیت**: مدل‌های بزرگتر معمولاً کیفیت ترجمه بهتری ارائه می‌دهند

## تنظیمات پیشرفته

### تغییر پورت Ollama
```bash
# راه‌اندازی Ollama روی پورت متفاوت
OLLAMA_HOST=0.0.0.0:11435 ollama serve
```

### استفاده از مدل‌های Quantized
```bash
# دانلود نسخه quantized برای صرفه‌جویی در حافظه
ollama pull gemma3:4b-q4_K_M
```

### تنظیمات Model Options
در فایل تنظیمات extension می‌توانید پارامترهای مدل را تغییر دهید:
- `temperature`: کنترل خلاقیت (0.1 برای ترجمه)
- `top_p`: کنترل تنوع خروجی
- `max_tokens`: حداکثر تعداد توکن‌های خروجی 